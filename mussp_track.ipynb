{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision.transforms.functional as T\n",
    "import torch\n",
    "from torchvision.ops import masks_to_boxes\n",
    "from torchvision.ops import nms\n",
    "from torchvision.ops.boxes import box_iou\n",
    "from groundingdino.util.inference import load_model, load_image\n",
    "from groundingdino.util.inference import predict as predict_gd\n",
    "import clip\n",
    "from tqdm import tqdm\n",
    "import linecache\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "\n",
    "import cv2\n",
    "import gc\n",
    "import os\n",
    "import copy\n",
    "import sys\n",
    "import re\n",
    "from PIL import Image\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from torch.distributions import kl_divergence\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "\n",
    "# ######################### muSSP import #################################\n",
    "import mot3d\n",
    "import mot3d.weight_functions as wf\n",
    "from mot3d.utils import utils\n",
    "# ########################################################################\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Get GD\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "gd_model = load_model(\n",
    "    \"../GroundingDINO/groundingdino/config/GroundingDINO_SwinB_cfg.py\",\n",
    "    \"../GroundingDINO/weights/groundingdino_swinb_cogcoor.pth\")\n",
    "\n",
    "# Get SAM\n",
    "sam_checkpoint = \"/home/jash/Desktop/sam_vit_h_4b8939.pth\"\n",
    "model_type = \"vit_h\"\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)\n",
    "\n",
    "sam_predictor = SamPredictor(sam)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle_to_mask(rle, shape):\n",
    "    mask = np.zeros(shape[0] * shape[1], dtype=bool)\n",
    "    current_pos = 0\n",
    "    for idx, length in enumerate(rle):\n",
    "        color = idx % 2 == 1\n",
    "        mask[current_pos: current_pos + length] = color\n",
    "        current_pos += length\n",
    "    return mask.reshape(shape)\n",
    "\n",
    "\n",
    "def get_multiple_boolean_masks(example_name, image_shape, frame=1):\n",
    "    masks = []\n",
    "    folder_path = '/home/jash/Desktop/vot_workplace/sequences/' + example_name\n",
    "    ground_truth_files = [f for f in os.listdir(\n",
    "        folder_path) if f.startswith('groundtruth') and f.endswith('.txt')]\n",
    "\n",
    "    for gt_file in ground_truth_files:\n",
    "        gt_file_path = os.path.join(folder_path, gt_file)\n",
    "        mask_str = linecache.getline(gt_file_path, frame).strip('\\n')\n",
    "\n",
    "        # Parse the mask string\n",
    "        values = [int(val) for val in mask_str[1:].split(',')]\n",
    "        offset_x, offset_y, region_width, region_height = values[:4]\n",
    "        rle_values = values[4:]\n",
    "\n",
    "        # Convert the RLE to a boolean mask\n",
    "        region_mask = rle_to_mask(rle_values, (region_height, region_width))\n",
    "\n",
    "        # Create a full-sized boolean mask with the region mask at the correct\n",
    "        # offset\n",
    "        boolean_mask = np.zeros(image_shape, dtype=bool)\n",
    "        boolean_mask[offset_y: offset_y + region_height,\n",
    "                     offset_x: offset_x + region_width] = region_mask\n",
    "\n",
    "        masks.append(boolean_mask)\n",
    "\n",
    "    return masks\n",
    "\n",
    "\n",
    "def boolean_mask_to_bbox(boolean_mask):\n",
    "\n",
    "    # Check if mask is int8 if not convert it\n",
    "    boolean_mask = torch.tensor(boolean_mask)\n",
    "\n",
    "    if boolean_mask.dtype != torch.uint8:\n",
    "        binary_mask = boolean_mask.to(torch.uint8)\n",
    "    else:\n",
    "        binary_mask = boolean_mask\n",
    "\n",
    "    # Add an extra dimension to binary_mask to match the expected input shape\n",
    "    # for masks_to_boxes\n",
    "    binary_mask = binary_mask.unsqueeze(0)\n",
    "    # Compute the bounding box\n",
    "    bbox = masks_to_boxes(binary_mask)\n",
    "\n",
    "    return bbox.squeeze(0)\n",
    "\n",
    "\n",
    "def get_crop(img, obj_dimensions=None, scale=1):\n",
    "    \"\"\"Given top left cooridnate and height and width of the crop,\n",
    "    it returns a squared version of the crop and scales it, if required.\n",
    "\n",
    "    Args:\n",
    "        example_name (string): Name of the example from the VOT dataset.\n",
    "        scale (int, optional): Parameter to scale square size. Defaults to 1.\n",
    "        Can't be set to 0.\n",
    "\n",
    "    \"\"\"\n",
    "    if scale == 0:\n",
    "        raise ValueError(\"Scale must be > 0\")\n",
    "\n",
    "    def _resized_square(x, y, h, w, scale=1):\n",
    "        \"\"\"\n",
    "         Args:\n",
    "            x (float): Horizontal component of the top left corner of the crop box.\n",
    "            y (float): Vertical component of the top left corner of the crop box.\n",
    "            h (float): Height of the crop box.\n",
    "            w (float): Width of the crop box.\n",
    "            scale (int, optional): Parameter to scale square size. Defaults to 1.\n",
    "            Can't be set to 0.\n",
    "\n",
    "        \"\"\"\n",
    "        midx, midy = (2 * x + w) // 2, (2 * y + h) // 2\n",
    "        new_square_len = max(w, h) * scale\n",
    "        x = midx - new_square_len // 2\n",
    "        y = midy - new_square_len // 2\n",
    "        w = h = new_square_len\n",
    "        return x, y, h, w\n",
    "\n",
    "    # User defined obj dimensions\n",
    "    obj_x, obj_y, obj_h, obj_w = np.array(\n",
    "        [obj_dimensions[0],\n",
    "         obj_dimensions[1],\n",
    "         obj_dimensions[3] - obj_dimensions[1],\n",
    "         obj_dimensions[2] - obj_dimensions[0]])\n",
    "\n",
    "    # Changes top left coordinates, h and w of ground truth and scales the\n",
    "    # square.\n",
    "    resized_x, resized_y, resized_h, resized_w = _resized_square(\n",
    "        obj_x, obj_y, obj_h, obj_w, scale=scale)\n",
    "    # Crops image with given parameters\n",
    "    cropped_image = T.crop(Image.fromarray(np.uint8(img)), int(resized_y), int(\n",
    "        resized_x), int(resized_h), int(resized_w))\n",
    "\n",
    "    return cropped_image\n",
    "\n",
    "\n",
    "def get_bbox_iou(box, boxes):\n",
    "    boxes1 = np.array(box).reshape(1, 4)\n",
    "    boxes2 = np.array(boxes)\n",
    "\n",
    "    x11, y11, x12, y12 = np.split(boxes1, 4, axis=1)\n",
    "    x21, y21, x22, y22 = np.split(boxes2, 4, axis=1)\n",
    "\n",
    "    xA = np.maximum(x11, np.transpose(x21))\n",
    "    yA = np.maximum(y11, np.transpose(y21))\n",
    "    xB = np.minimum(x12, np.transpose(x22))\n",
    "    yB = np.minimum(y12, np.transpose(y22))\n",
    "\n",
    "    interArea = np.maximum((xB - xA + 1), 0) * np.maximum((yB - yA + 1), 0)\n",
    "    boxAArea = (x12 - x11 + 1) * (y12 - y11 + 1)\n",
    "    boxBArea = (x22 - x21 + 1) * (y22 - y21 + 1)\n",
    "\n",
    "    iou = interArea / (boxAArea + np.transpose(boxBArea) - interArea)\n",
    "    return iou\n",
    "\n",
    "\n",
    "def boxes_distance(boxA, boxB):\n",
    "    # boxA and boxB are tuples in the format (x1, y1, x2, y2)\n",
    "\n",
    "    A_min = np.array(boxA[:2])  # x1, y1\n",
    "    A_max = np.array(boxA[2:])  # x2, y2\n",
    "    B_min = np.array(boxB[:2])  # x1, y1\n",
    "    B_max = np.array(boxB[2:])  # x2, y2\n",
    "\n",
    "    delta1 = A_min - B_min  # difference between left/top\n",
    "    delta2 = A_max - B_max  # difference between right/bottom\n",
    "    dist = np.sum(np.abs(np.concatenate([delta1, delta2])))\n",
    "\n",
    "    return dist\n",
    "\n",
    "\n",
    "def linear_assignment(cost_matrix):\n",
    "    \"\"\"\n",
    "    Hungarian algorithm for solving assignment problem.\n",
    "    \"\"\"\n",
    "    # try:\n",
    "    #     _, x, y = lap.lapjv(cost_matrix, extend_cost=True)\n",
    "    #     valid_assignments = x != -1\n",
    "    #     return np.arange(len(x))[valid_assignments], x[valid_assignments]\n",
    "    # except BaseException:\n",
    "    x, y = linear_sum_assignment(cost_matrix)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def get_indices_with_low_kl_div(single_prob_dist, prob_dist_array, threshold):\n",
    "    \"\"\"\n",
    "    Given a single probability distribution, an array of probability distributions,\n",
    "    and a threshold, compute the KL divergence between the single distribution\n",
    "    and each distribution in the array, and return the indices of those whose\n",
    "    KL divergence is lower than the threshold.\n",
    "\n",
    "    Args:\n",
    "    single_prob_dist (torch.Tensor): Single probability distribution of shape [n,].\n",
    "    prob_dist_array (torch.Tensor): Array of probability distributions of shape [m, n].\n",
    "    threshold (float): KL divergence threshold.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Indices of the prob_dist_array whose KL divergence with single_prob_dist is lower than threshold.\n",
    "    \"\"\"\n",
    "    non_zero_indices = torch.nonzero(single_prob_dist, as_tuple=True)[0]\n",
    "\n",
    "    single_prob_dist = single_prob_dist[non_zero_indices]\n",
    "    prob_dist_array = prob_dist_array[:, non_zero_indices]\n",
    "\n",
    "    single_cat = Categorical(single_prob_dist)\n",
    "    prob_dist_cats = Categorical(prob_dist_array)\n",
    "\n",
    "    kl_divs = kl_divergence(single_cat, prob_dist_cats)\n",
    "    low_kl_indices = torch.where(kl_divs < threshold)[0]\n",
    "\n",
    "    return low_kl_indices.tolist(), kl_divs\n",
    "\n",
    "\n",
    "def plot_results(img_path, tracked_boxes, sequence, plot_name=\"\", tracked_masks=None):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    img = cv2.imread(img_path)[:, :, ::-1]\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"{plot_name}:: Sequence: {sequence}, Frame: {img_path.split('/')[-1]}\")\n",
    "\n",
    "    ax = plt.gca()\n",
    "\n",
    "    colours = [cm.viridis(i/len(tracked_boxes))[:-1] for i in range(len(tracked_boxes))]\n",
    "    for object_id, bbox in tracked_boxes:\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        ax.add_patch(\n",
    "            plt.Rectangle(\n",
    "                (x1,\n",
    "                 y1),\n",
    "                x2 - x1,\n",
    "                y2 - y1,\n",
    "                fill=False,\n",
    "                color=colours[object_id],\n",
    "                linewidth=3))\n",
    "        ax.text(x1, y1 - 2, f\"ID: {object_id}\", color='r', fontsize=12)\n",
    "\n",
    "    if tracked_masks is not None:\n",
    "        for object_id, mask in tracked_masks:\n",
    "            mask = mask.squeeze()\n",
    "            # Overlay the mask on the image\n",
    "            ax.imshow(\n",
    "                np.ma.masked_where(\n",
    "                    mask == 0,\n",
    "                    mask),\n",
    "                alpha=0.5,\n",
    "                cmap='jet',\n",
    "                interpolation='none')\n",
    "\n",
    "    # Create folder if folder does not exist\n",
    "    if not os.path.exists(f\"{plot_name}_results/{sequence}\"):\n",
    "        os.makedirs(f\"{plot_name}_results/{sequence}\")\n",
    "    plt.savefig(f\"{plot_name}_results/{sequence}/{img_path.split('/')[-1]}\")\n",
    "    plt.close()\n",
    "\n",
    "def debug_plot(image_n, sequence, img_path, detection_bboxes, prev_predicted_boxes):\n",
    "    # Plot prev predicted boxes in green and detection boxes in red on same\n",
    "    # image\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image_n)\n",
    "    red_patch = patches.Patch(color='red', label='GD detections')\n",
    "    green_patch = patches.Patch(color='green', label='prev predicted')\n",
    "    plt.legend(handles=[red_patch, green_patch])\n",
    "\n",
    "    plt.title(f\"Sequence: {sequence}, Frame: {img_path.split('/')[-1]}\")\n",
    "    for bbox in prev_predicted_boxes:\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        plt.gca().add_patch(\n",
    "            plt.Rectangle(\n",
    "                (x1,\n",
    "                 y1),\n",
    "                x2 - x1,\n",
    "                y2 - y1,\n",
    "                fill=False,\n",
    "                color='g',\n",
    "                linewidth=3))\n",
    "    for i, bbox in enumerate(detection_bboxes):\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        plt.gca().add_patch(\n",
    "            plt.Rectangle(\n",
    "                (x1,\n",
    "                 y1),\n",
    "                x2 - x1,\n",
    "                y2 - y1,\n",
    "                fill=False,\n",
    "                color='r',\n",
    "                linewidth=1))\n",
    "        plt.text(x1, y1 - 2, f\"ID:{i}\", color='r', fontsize=10)\n",
    "\n",
    "    # Create folder\n",
    "    if not os.path.exists(f\"debug/{sequence}\"):\n",
    "        os.makedirs(f\"debug/{sequence}\")\n",
    "    plt.savefig(f\"debug/{sequence}/tracked_{img_path.split('/')[-1]}\")\n",
    "    plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAM setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_to_seg(image, boxes):\n",
    "    sam_predictor.set_image(image)\n",
    "\n",
    "    input_boxes = torch.tensor(boxes, device=sam_predictor.device)\n",
    "    transformed_boxes = sam_predictor.transform.apply_boxes_torch(\n",
    "        input_boxes, image.shape[: 2])\n",
    "    masks, mask_quality, _ = sam_predictor.predict_torch(\n",
    "        point_coords=None,\n",
    "        point_labels=None,\n",
    "        boxes=transformed_boxes,\n",
    "        multimask_output=True\n",
    "    )\n",
    "    # masks has shape (n, 3, w, h), where n is the number of objects, 3 is the number of masks for each box, and w, h are the width and height of the image.\n",
    "    # mask_quality has shape (n, 3), where n is the number of objects, and 3 is the number of masks for each box.\n",
    "    # Instead of returning all 3 masks, we return the mask with the highest\n",
    "    # quality.\n",
    "    best_mask_indices = torch.argmax(mask_quality, dim=1)\n",
    "    masks = masks[torch.arange(masks.shape[0]), best_mask_indices]\n",
    "\n",
    "    return masks.squeeze(1).cpu().numpy()\n",
    "\n",
    "\n",
    "def box_to_seg_2(image, boxes):\n",
    "    sam_predictor.set_image(image)\n",
    "\n",
    "    input_boxes = torch.tensor(boxes, device=sam_predictor.device)\n",
    "    transformed_boxes = sam_predictor.transform.apply_boxes_torch(\n",
    "        input_boxes, image.shape[: 2])\n",
    "    masks, _, _ = sam_predictor.predict_torch(\n",
    "        point_coords=None,\n",
    "        point_labels=None,\n",
    "        boxes=transformed_boxes,\n",
    "        multimask_output=False,\n",
    "    )\n",
    "    return masks.squeeze(1).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence = \"f1\"\n",
    "# img_dir = f\"/home/jash/Desktop/vot_workplace/sequences/{sequence}/color/\"\n",
    "# img_path_list = sorted([os.path.join(img_dir, img_name)\n",
    "#                         for img_name in os.listdir(img_dir)\n",
    "#                         if img_name.endswith('.jpg')])\n",
    "# image_shape = cv2.imread(img_path_list[0]).shape\n",
    "# gt_masks = get_multiple_boolean_masks(sequence, image_shape[:2])\n",
    "# gt_bboxes = [boolean_mask_to_bbox(mask).tolist() for mask in gt_masks]\n",
    "\n",
    "# seg_masks = box_to_seg(cv2.imread(img_path_list[0])[:, :, ::-1], gt_bboxes)\n",
    "# # Plotting\n",
    "# fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "# ax.imshow(cv2.imread(img_path_list[0])[:, :, ::-1])\n",
    "# for box in gt_bboxes:\n",
    "#     x1, y1, x2, y2 = box\n",
    "#     ax.add_patch(\n",
    "#         patches.Rectangle(\n",
    "#             (x1,\n",
    "#              y1),\n",
    "#             x2 - x1,\n",
    "#             y2 - y1,\n",
    "#             fill=False,\n",
    "#             color='g',\n",
    "#             linewidth=3))\n",
    "# for mask in seg_masks:\n",
    "#     ax.imshow(\n",
    "#         np.ma.masked_where(\n",
    "#             mask == 0,\n",
    "#             mask),\n",
    "#         alpha=0.5,\n",
    "#         cmap='jet',\n",
    "#         interpolation='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given 2 masks of same shape, write function to calculate the IoU between them\n",
    "def calculate_mask_iou(mask1, mask2):\n",
    "    intersection = np.logical_and(mask1, mask2)\n",
    "    union = np.logical_or(mask1, mask2)\n",
    "    iou_score = np.sum(intersection) / np.sum(union)\n",
    "    return iou_score\n",
    "\n",
    "\n",
    "#  For all sequences in the VOT dataset, calculate the IoU between the ground truth masks and the segmentation masks for first frame.\n",
    "# Plot mIou scores for each sequence print average mIoU score for all sequences.\n",
    "# mIoU_scores = []\n",
    "# sequence_names = []\n",
    "# for sequence in tqdm(os.listdir(\"/home/jash/Desktop/vot_workplace/sequences/\")):\n",
    "#     if sequence == \"list.txt\":\n",
    "#         continue\n",
    "#     sequence_names.append(sequence)\n",
    "#     img_dir = f\"/home/jash/Desktop/vot_workplace/sequences/{sequence}/color/00000001.jpg\"\n",
    "#     image_shape = cv2.imread(img_dir).shape\n",
    "#     gt_masks = get_multiple_boolean_masks(sequence, image_shape[:2])\n",
    "#     gt_bboxes = [boolean_mask_to_bbox(mask).tolist() for mask in gt_masks]\n",
    "#     seg_masks = box_to_seg(cv2.imread(img_dir)[:, :, ::-1], gt_bboxes)\n",
    "#     iou_scores = []\n",
    "#     for i in range(len(gt_masks)):\n",
    "#         iou_scores.append(calculate_mask_iou(gt_masks[i], seg_masks[i]))\n",
    "#     mIoU_scores.append(np.mean(iou_scores))\n",
    "\n",
    "# fig = plt.figure(figsize=(20, 10))\n",
    "# plt.bar(sequence_names, mIoU_scores)\n",
    "# plt.title(\"mIoU scores for all sequences\")\n",
    "# plt.show()\n",
    "# print(f\"Average mIoU score for all sequences: {np.mean(mIoU_scores)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GD setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def transform_gd_boxes(boxes, image_shape):\n",
    "    \"\"\"\n",
    "    Transforms the boxes from GD format to x1,y1,x2,y2 format.\n",
    "    \"\"\"\n",
    "    h, w, _ = image_shape\n",
    "    boxes = boxes * torch.Tensor([w, h, w, h])\n",
    "    cx, cy, w, h = boxes.unbind(-1)\n",
    "    x1 = cx - 0.5 * w\n",
    "    y1 = cy - 0.5 * h\n",
    "    x2 = cx + 0.5 * w\n",
    "    y2 = cy + 0.5 * h\n",
    "\n",
    "    return torch.stack((x1, y1, x2, y2), dim=-1).type(torch.int)\n",
    "\n",
    "\n",
    "custom = \"object. wire crossing. frisbee. disk. person. cone. motorcycle.\\\n",
    " head. insect. animal. sea creature. fish. game character. shape. object in the dark. person in the dark.\\\n",
    " skateboard. bag. plastic. white thing. black thing. crab. bird. sports person. two wheeler. penguin. boat. book. bottle.\\\n",
    " vehicle. reptile. hat. shoe. ball. object in focus. object in center. human. flying object. flag. \\\n",
    " head of object. hand of human. kite. leaf. car. licenseplate. machine. robot. equipment. part. umbrella.\"\n",
    "\n",
    "\n",
    "def detect_objects(\n",
    "        img_path,\n",
    "        box_threshold=0.08,\n",
    "        text_threshold=0.25,\n",
    "        caption=custom):\n",
    "\n",
    "    _, image = load_image(img_path)\n",
    "\n",
    "    boxes, box_scores, _, all_logits,_ = predict_gd(\n",
    "        model=gd_model,\n",
    "        image=image,\n",
    "        caption=caption,\n",
    "        box_threshold=box_threshold,\n",
    "        text_threshold=text_threshold\n",
    "    )\n",
    "    return boxes, box_scores, all_logits\n",
    "\n",
    "\n",
    "def get_gt_prob_dist(bbox, frame_1_detections, frame_1_logits):\n",
    "    \"\"\"\n",
    "    Get gt prob distribution by taking max iou between 1st frame gt\n",
    "    box and 1st frame detections.\n",
    "    Return the prob distribution of the box with max iou.\n",
    "\n",
    "    \"\"\"\n",
    "    iou_values = get_bbox_iou(bbox, frame_1_detections)\n",
    "    max_iou_index = np.argmax(iou_values)\n",
    "    return frame_1_logits[max_iou_index], max_iou_index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Tracker setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_assignment(cost_matrix):\n",
    "    \"\"\"\n",
    "    Hungarian algorithm for solving assignment problem.\n",
    "    \"\"\"\n",
    "    # try:\n",
    "    #     _, x, y = lap.lapjv(cost_matrix, extend_cost=True)\n",
    "    #     valid_assignments = x != -1\n",
    "    #     return np.arange(len(x))[valid_assignments], x[valid_assignments]\n",
    "    # except BaseException:\n",
    "    x, y = linear_sum_assignment(cost_matrix)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def KL_filter(detection_prob_distributions, kl_threshold=0.3):\n",
    "\n",
    "    global trackers  # Use the global trackers dictionary\n",
    "    # For each track, we will compute the KL divergence between the track's\n",
    "    # p_gt and detection_prob_distributions.\n",
    "\n",
    "    detections_sim_for_each_track = []\n",
    "    KL_matrix = []\n",
    "    # Iterate only non-distractors\n",
    "    for track in trackers.values():\n",
    "        top_div, all_div = get_indices_with_low_kl_div(\n",
    "            track['prob_dist'], detection_prob_distributions, kl_threshold)\n",
    "        detections_sim_for_each_track.append(top_div)\n",
    "        KL_matrix.append(all_div.tolist())\n",
    "    # The union of all the indices.\n",
    "    unioned_indices = list(set().union(*detections_sim_for_each_track))\n",
    "    # Filtering the KL matrix to only contain the unioned indices.\n",
    "    KL_matrix = np.array(KL_matrix)\n",
    "    unioned_matrix = KL_matrix[np.arange(KL_matrix.shape[0])[\n",
    "        :, None], unioned_indices]\n",
    "\n",
    "    return unioned_indices, unioned_matrix\n",
    "\n",
    "\n",
    "def iou(bb_test, bb_gt):\n",
    "    \"\"\"\n",
    "    From SORT: Computes IOU between two bboxes in the form [x1,y1,x2,y2]\n",
    "    \"\"\"\n",
    "    bb_gt = np.expand_dims(bb_gt, 0)\n",
    "    bb_test = np.expand_dims(bb_test, 1)\n",
    "\n",
    "    xx1 = np.maximum(bb_test[..., 0], bb_gt[..., 0])\n",
    "    yy1 = np.maximum(bb_test[..., 1], bb_gt[..., 1])\n",
    "    xx2 = np.minimum(bb_test[..., 2], bb_gt[..., 2])\n",
    "    yy2 = np.minimum(bb_test[..., 3], bb_gt[..., 3])\n",
    "    w = np.maximum(0., xx2 - xx1)\n",
    "    h = np.maximum(0., yy2 - yy1)\n",
    "    wh = w * h\n",
    "    o = wh / ((bb_test[...,\n",
    "                       2] - bb_test[...,\n",
    "                                    0]) * (bb_test[...,\n",
    "                                                   3] - bb_test[...,\n",
    "                                                                1]) + (bb_gt[...,\n",
    "                                                                             2] - bb_gt[...,\n",
    "                                                                                        0]) * (bb_gt[...,\n",
    "                                                                                                     3] - bb_gt[...,\n",
    "                                                                                                                1]) - wh)\n",
    "    return -(o)\n",
    "\n",
    "\n",
    "def custom_cost(boxes1, boxes2, image_shape, alpha=0.5):\n",
    "    # Calculate center of each box\n",
    "    center1 = np.column_stack(\n",
    "        [(boxes1[:, 0] + boxes1[:, 2]) / 2, (boxes1[:, 1] + boxes1[:, 3]) / 2])\n",
    "    center2 = np.column_stack(\n",
    "        [(boxes2[:, 0] + boxes2[:, 2]) / 2, (boxes2[:, 1] + boxes2[:, 3]) / 2])\n",
    "\n",
    "    # Expand dimensions for broadcasting\n",
    "    center1_exp = np.expand_dims(center1, axis=1)\n",
    "    center2_exp = np.expand_dims(center2, axis=0)\n",
    "\n",
    "    # Calculate L2 distance between centers\n",
    "    l2_dist = np.sqrt(np.sum((center1_exp - center2_exp) ** 2, axis=2))\n",
    "    # Normalize l2_dist by longest side of the image\n",
    "    l2_dist = l2_dist / np.max(image_shape)\n",
    "\n",
    "    # Calculate area of each box\n",
    "    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n",
    "    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n",
    "\n",
    "    # Expand dimensions for broadcasting\n",
    "    area1_exp = np.expand_dims(area1, axis=1)\n",
    "    area2_exp = np.expand_dims(area2, axis=0)\n",
    "\n",
    "    # Compute difference in log areas\n",
    "    log_area_diff = np.abs(np.log(area1_exp) - np.log(area2_exp))\n",
    "    # print(log_area_diff)\n",
    "\n",
    "    # Calculate cost_dist\n",
    "    cost_dist = l2_dist + log_area_diff\n",
    "\n",
    "    # Add penalty for pairs with normalized L2 distance less than 0.1\n",
    "    cost_dist[l2_dist < 0.1] += 1\n",
    "    # Add penalty for pairs with log area difference greater than 0.3\n",
    "    cost_dist[log_area_diff > 0.3] += 1\n",
    "\n",
    "    return cost_dist\n",
    "\n",
    "\n",
    "def l2_pairwise(boxes1, boxes2, image_shape):\n",
    "    # Calculate center of each box\n",
    "    center1 = np.column_stack(\n",
    "        [(boxes1[:, 0] + boxes1[:, 2]) / 2, (boxes1[:, 1] + boxes1[:, 3]) / 2])\n",
    "    center2 = np.column_stack(\n",
    "        [(boxes2[:, 0] + boxes2[:, 2]) / 2, (boxes2[:, 1] + boxes2[:, 3]) / 2])\n",
    "\n",
    "    # Expand dimensions for broadcasting\n",
    "    center1_exp = np.expand_dims(center1, axis=1)\n",
    "    center2_exp = np.expand_dims(center2, axis=0)\n",
    "\n",
    "    # Calculate L2 distance between centers\n",
    "    l2_dist = np.sqrt(np.sum((center1_exp - center2_exp) ** 2, axis=2))\n",
    "    # Normalize l2_dist by longest side of the image\n",
    "    l2_dist = l2_dist / np.max(image_shape)\n",
    "\n",
    "    # # Add penalty for pairs with normalized L2 distance less than 0.1\n",
    "    # l2_dist[l2_dist < 0.1] *= 10\n",
    "    return l2_dist\n",
    "\n",
    "\n",
    "def giou_pairwise(boxes1, boxes2):\n",
    "\n",
    "    # Calculate intersection and union\n",
    "    x1 = np.maximum(boxes1[:, None, 0], boxes2[:, 0])\n",
    "    y1 = np.maximum(boxes1[:, None, 1], boxes2[:, 1])\n",
    "    x2 = np.minimum(boxes1[:, None, 2], boxes2[:, 2])\n",
    "    y2 = np.minimum(boxes1[:, None, 3], boxes2[:, 3])\n",
    "\n",
    "    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n",
    "    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n",
    "    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n",
    "    union = area1[:, None] + area2 - intersection\n",
    "\n",
    "    # Calculate enclosing box\n",
    "    cx1 = np.minimum(boxes1[:, None, 0], boxes2[:, 0])\n",
    "    cy1 = np.minimum(boxes1[:, None, 1], boxes2[:, 1])\n",
    "    cx2 = np.maximum(boxes1[:, None, 2], boxes2[:, 2])\n",
    "    cy2 = np.maximum(boxes1[:, None, 3], boxes2[:, 3])\n",
    "\n",
    "    c_area = (cx2 - cx1) * (cy2 - cy1)\n",
    "\n",
    "    iou = intersection / union\n",
    "    giou = iou - (c_area - union) / c_area\n",
    "\n",
    "    return -giou\n",
    "\n",
    "\n",
    "def associate_detections_to_trackers(\n",
    "        detections,\n",
    "        prev_predicted_boxes,\n",
    "        kl_matrix,\n",
    "        image_shape,\n",
    "        cost_alpha=0.5):\n",
    "    \"\"\"\n",
    "    Assigns detections to tracked object (both represented as bounding boxes)\n",
    "\n",
    "    Returns 3 lists of matches, unmatched_detections and unmatched_trackers\n",
    "    \"\"\"\n",
    "    # Trackers here is simply the terminology for\n",
    "    # a tracker on a particular \"object we want to track\".\n",
    "\n",
    "    # print(detections.numpy().shape, np.array(prev_predicted_boxes).shape)\n",
    "\n",
    "    \n",
    "    # giou_pairwise(\n",
    "    #     detections.numpy(),\n",
    "    #     np.array(prev_predicted_boxes))\\\n",
    "    score_matrix = np.sqrt((l2_pairwise(\n",
    "        detections.numpy(),\n",
    "        np.array(prev_predicted_boxes),\n",
    "        image_shape[: 2])) \\\n",
    "    * (kl_matrix * cost_alpha))\n",
    "\n",
    "    # print(detections.numpy().shape, np.array(prev_predicted_boxes).shape)\n",
    "\n",
    "    # Hungarian algorithm\n",
    "    matched_indices = linear_assignment(score_matrix)\n",
    "    if isinstance(matched_indices, tuple):\n",
    "        matched_indices = np.array(matched_indices)\n",
    "\n",
    "    # Matched indices has shape (2, num_matches)\n",
    "    # First row is the indices of the detections\n",
    "    # Second row is the indices of the trackers\n",
    "\n",
    "    # Generates two sets for each of the detections and trackers that\n",
    "    # contain their indices. Then we subtract the set of matched indices.\n",
    "    # The result is the indices of unmatched detections and trackers.\n",
    "    unmatched_detections = list(\n",
    "        set(range(len(detections))) - set(matched_indices[0]))\n",
    "    unmatched_trackers = list(\n",
    "        set(range(len(trackers))) - set(matched_indices[1]))\n",
    "\n",
    "    return matched_indices.T, np.array(\n",
    "        unmatched_detections), np.array(unmatched_trackers)\n",
    "\n",
    "\n",
    "def initialize_trackers(bboxes, frame_1_path, image_shape, NMS_THRESHOLD=0.3):\n",
    "    \"\"\"\n",
    "    Initialises each object's tracker using the initial bounding box.\n",
    "    \"\"\"\n",
    "    global trackers  # Use the global trackers dictionary\n",
    "    frame_1_detections, frame_1_confidences, frame_1_logits = detect_objects(\n",
    "        img_path=frame_1_path)\n",
    "    frame_1_detections = transform_gd_boxes(frame_1_detections, image_shape)\n",
    "    nms_filtered_indices = nms(\n",
    "        frame_1_detections.float(),\n",
    "        frame_1_confidences,\n",
    "        iou_threshold=NMS_THRESHOLD)\n",
    "    frame_1_detections = frame_1_detections[nms_filtered_indices]\n",
    "    frame_1_confidences = frame_1_confidences[nms_filtered_indices]\n",
    "    frame_1_logits = frame_1_logits[nms_filtered_indices]\n",
    "    \n",
    "    # Initialising GT objects\n",
    "    gt_detection_indexes = []\n",
    "    for object_id, bbox in enumerate(bboxes):\n",
    "        p_gt, detection_index = get_gt_prob_dist(\n",
    "            bbox,\n",
    "            frame_1_detections,\n",
    "            frame_1_logits)\n",
    "        gt_detection_indexes.append(detection_index)\n",
    "        trackers[object_id] = {\n",
    "            'prev': bbox,\n",
    "            'prob_dist': p_gt,\n",
    "            'distractor': False}\n",
    "    # Adding distractor objects to trackers\n",
    "    for i in range(len(frame_1_detections)):\n",
    "        if i not in gt_detection_indexes:\n",
    "            trackers[len(trackers)] = {\n",
    "                'prev': frame_1_detections[i].tolist(),\n",
    "                'prob_dist': frame_1_logits[i],\n",
    "                'distractor': True}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## muSSP association"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trajectory_data(trajectories, association_window):\n",
    "\ttrack_data = {}\n",
    "\t# print(50*\"#\")\n",
    "\tfor track in trajectories:\n",
    "\t\t# Ensure track has start and end, this is to consider only complete tracks.\n",
    "\t\t# This will ensure that len of trajectories (objects being tracked) = min(start objects, end objects)\n",
    "\t\tif track[0].id == 0 and track[-1].id == association_window-1:\n",
    "\t\t\tfor detection in track:\n",
    "\t\t\t\t# print(\"detection:\", detection.bbox)\n",
    "\t\t\t\tif int(detection.id) in track_data:\n",
    "\t\t\t\t\ttrack_data[int(detection.id)]+=[detection.bbox]\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\ttrack_data[int(detection.id)]=[detection.bbox]\n",
    "\treturn track_data\n",
    "\n",
    "def muSSP_association(detections_data):\n",
    "\t\"\"\"Performs multi-target association using the muSSP algorithm.\n",
    "\tdetections_data is a list of shape (global_association_window, no. of detections (dict))\n",
    "\t\"\"\"\n",
    "\tassociation_window = len(detections_data)\n",
    "\t# Extracting bounding boxes and prob_dists\n",
    "\tbounding_boxes = np.array([np.array([d[key]['prev']+[key] for key in d]) for d in detections_data])\n",
    "\tprob_dists = np.array([[d[key]['prob_dist'] for key in d] for d in detections_data])\n",
    "\n",
    "\t# calculate 2D centroids\n",
    "\tcentroids = np.array([np.array([[(box[0]+box[2])/2, (box[1]+box[3])/2] for box in frame]) for frame in bounding_boxes])\n",
    "\t\n",
    "\n",
    "\tdetections = [\n",
    "\t\t\tmot3d.Detection2D(index, position, bbox=box_[:4], id=box_[4])\n",
    "\t\t\tfor index, (centroid_list, bbox_list) in enumerate(zip(centroids, bounding_boxes))\n",
    "\t\t\tfor position, box_ in zip(centroid_list, bbox_list)\n",
    "\t\t\t]\n",
    "\t\n",
    "\n",
    "\tweight_distance = lambda d1, d2: wf.weight_distance_detections_2d(d1, d2,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tsigma_jump=1, sigma_distance=10,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tsigma_color_histogram=0.3, sigma_box_size=0.3,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tmax_distance=15,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tuse_color_histogram=False, use_bbox=True)\n",
    "\tweight_confidence = lambda d: wf.weight_confidence_detections_2d(d, mul=1, bias=0)\n",
    "\n",
    "\t# print(\"Given:\", len(detections))\n",
    "\tg = mot3d.build_graph(detections, weight_source_sink=0.1,\n",
    "\t\t\t\t\tmax_jump=2, verbose=0,\n",
    "\t\t\t\t\tweight_confidence=weight_confidence,\n",
    "\t\t\t\t\tweight_distance=weight_distance)\n",
    "\ttrajectories = mot3d.solve_graph(g, verbose=0, method='muSSP')  \n",
    "\t# print(\"received:\", len(trajectories))\n",
    "\ttrack_data = get_trajectory_data(trajectories, association_window)\n",
    "\t# print(\"len of track data:\",len(track_data))\n",
    "\treturn track_data, trajectories"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:00<00:03,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "# of dummy edges : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_472165/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">3388788059.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">119</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_472165/3388788059.py'</span>                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">IndexError: </span>too many indices for array: array is <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>-dimensional, but <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> were indexed\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_472165/\u001b[0m\u001b[1;33m3388788059.py\u001b[0m:\u001b[94m119\u001b[0m in \u001b[92m<module>\u001b[0m                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_472165/3388788059.py'\u001b[0m                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mIndexError: \u001b[0mtoo many indices for array: array is \u001b[1;36m1\u001b[0m-dimensional, but \u001b[1;36m3\u001b[0m were indexed\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###################### Setup of global association using muSSP #########################\n",
    "global_association_window = 3 # Number of frames to associate\n",
    "global_association_detections = []\n",
    "# This dict will store the detections data for each frame in the global association window.\n",
    "# It's structure is described in the muSSP_association fxn.\n",
    "########################################################################################\n",
    "\n",
    "trackers = {}\n",
    "sequence = \"frisbee\"\n",
    "img_dir = f\"/home/jash/Desktop/vot_workplace/sequences/{sequence}/color/\"\n",
    "img_path_list = sorted([os.path.join(img_dir, img_name)\n",
    "                        for img_name in os.listdir(img_dir)\n",
    "                        if img_name.endswith('.jpg')])\n",
    "image_shape = cv2.imread(img_path_list[0]).shape\n",
    "\n",
    "# PARAMETERS\n",
    "GD_BOX_THRESHOLD = 0.07\n",
    "KL_THRESHOLD = 0.5\n",
    "NMS_THRESHOLD = 0.1\n",
    "\n",
    "# Get the ground truth bounding boxes\n",
    "gt_masks = get_multiple_boolean_masks(sequence, image_shape[:2])\n",
    "gt_bboxes = [boolean_mask_to_bbox(mask).tolist() for mask in gt_masks]\n",
    "\n",
    "# Initialization with the first frame\n",
    "initialize_trackers(gt_bboxes, img_path_list[0], image_shape, NMS_THRESHOLD=NMS_THRESHOLD)\n",
    "\n",
    "# print(30*\"*\")\n",
    "# for track in trackers:\n",
    "#     print(trackers[track]['prev'])\n",
    "# print(30*\"*\")\n",
    "\n",
    "global_association_detections.append(copy.deepcopy(trackers))\n",
    "\n",
    "# print(\"Detections added:\", len(trackers))\n",
    "# print(30*\"*\")\n",
    "# for dict in global_association_detections:\n",
    "#     # print(dict.keys())\n",
    "#     for detecion in dict.values():\n",
    "#         print(detecion['prev'])\n",
    "# print(30*\"*\")\n",
    "\n",
    "no_of_frames = 6\n",
    "for img_path in tqdm(img_path_list[1:no_of_frames]):\n",
    "    image_n = cv2.imread(img_path)[:, :, ::-1]\n",
    "    current_frame = int(img_path[-8:-4].lstrip('0'))\n",
    "\n",
    "    # Get detections and their probability distributions\n",
    "    boxes, detection_scores, detection_prob_distributions = detect_objects(\n",
    "        img_path, box_threshold=GD_BOX_THRESHOLD, text_threshold=0.25)\n",
    "    detection_bboxes = transform_gd_boxes(boxes, image_shape)\n",
    "\n",
    "    # 1. Filter detections using KL divergence and then NMS\n",
    "    # KL divergence\n",
    "    KL_filtered_indices, KL_matrix = KL_filter(\n",
    "        detection_prob_distributions, kl_threshold=KL_THRESHOLD)\n",
    "    detection_bboxes = detection_bboxes[KL_filtered_indices]\n",
    "    detection_scores = detection_scores[KL_filtered_indices]\n",
    "    detection_prob_distributions = detection_prob_distributions[KL_filtered_indices]\n",
    "\n",
    "    # NMS\n",
    "    nms_filtered_indices = nms(\n",
    "        detection_bboxes.float(),\n",
    "        detection_scores,\n",
    "        iou_threshold=NMS_THRESHOLD)\n",
    "    detection_bboxes = detection_bboxes[nms_filtered_indices]\n",
    "    detection_scores = detection_scores[nms_filtered_indices]\n",
    "    detection_prob_distributions = detection_prob_distributions[nms_filtered_indices]\n",
    "\n",
    "    # 2: Associate detections to predicted tracker locations\n",
    "    # Get previous locations for all trackers\n",
    "    # prev_predicted_boxes = [trackers[k]['prev'] for k in trackers]\n",
    "\n",
    "    ########################### DEBUG PLOTTING ###############################\n",
    "    # debug_plot(image_n, sequence, img_path, detection_bboxes, prev_predicted_boxes)\n",
    "    ##########################################################################\n",
    "\n",
    "\n",
    "    trackers = {}\n",
    "    for i, bbox in enumerate(detection_bboxes):\n",
    "        trackers[i] = {'prev': bbox.tolist(), 'prob_dist': detection_prob_distributions[i] ,'distractor': True}\n",
    "\n",
    "    # Collect the detections for the global association\n",
    "    global_association_detections.append(copy.deepcopy(trackers))\n",
    "    # print(30*\"*\")\n",
    "    # for dict in global_association_detections:\n",
    "    #     # print(dict.keys())\n",
    "    #     for detecion in dict.values():\n",
    "    #         print(detecion['prev'])\n",
    "    # print(30*\"*\")\n",
    "    # 3: Update matched trackers with assigned detection_bboxes\n",
    "    # print(current_frame)\n",
    "    # for match in sorted(matches, key=lambda x: x[1]):\n",
    "    #     # print(match)\n",
    "    #     tracker_id = match[1]\n",
    "    #     trackers[tracker_id]['prev'] = detection_bboxes[match[0]].tolist()\n",
    "    #     if trackers[tracker_id]['distractor']:\n",
    "    #         trackers[tracker_id]['prob_dist'] = detection_prob_distributions[match[0]]\n",
    "\n",
    "    \n",
    "    # 4: Remove trackers that lost their objec  ts\n",
    "    # for i in unmatched_trackers:\n",
    "    #     tracker_id = list(predicted_boxes.keys())[i]\n",
    "    #     del trackers[tracker_id]\n",
    "\n",
    "    if len(global_association_detections)%global_association_window==0:\n",
    "        track_data, trajectories = muSSP_association(global_association_detections)\n",
    "        track_data = np.array(list(track_data.values()))\n",
    "        print(track_data)\n",
    "        # print(track_data)\n",
    "        # muSSP Plot\n",
    "        plt.figure()\n",
    "        mot3d.plot_trajectories(trajectories)\n",
    "        plt.title(\"Output (muSSP)\")\n",
    "        plt.savefig(f\"muSSP_track_graphs/output_mussp_{current_frame}.jpg\")\n",
    "        plt.close()\n",
    "\n",
    "        for index, img_path in enumerate(img_path_list[(current_frame-global_association_window):current_frame]):\n",
    "            result = np.array([(i, tuple(bbox)) for i, bbox in enumerate(track_data[:, index, :].reshape(-1, 4))])\n",
    "            plot_results(img_path, result, sequence, plot_name=\"muSSP\")\n",
    "\n",
    "        # Reset the global association detections\n",
    "        global_association_detections = []\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_video_from_images(frame_folder, video_output, fps):\n",
    "    \"\"\"\n",
    "    Creates an MP4 video from images in a folder.\n",
    "\n",
    "    Args:\n",
    "    frame_folder (str): Path to the folder containing the image frames.\n",
    "    video_output (str): Filename for the output video (with .mp4 extension).\n",
    "    fps (int): Frames per second for the output video.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the frame size from the first image in the folder\n",
    "    def get_frame_number(filename):\n",
    "        match = re.search(r\"(\\d+)\", filename)\n",
    "        return int(match.group(1)) if match else None\n",
    "\n",
    "    image_files = [f for f in os.listdir(\n",
    "        frame_folder) if f.endswith((\".png\", \".jpg\", \".jpeg\"))]\n",
    "    image_files = sorted(image_files, key=get_frame_number)\n",
    "\n",
    "    first_image = os.path.join(frame_folder, image_files[0])\n",
    "    first_frame = cv2.imread(first_image)\n",
    "    frame_size = (first_frame.shape[1], first_frame.shape[0])\n",
    "\n",
    "    # Define the codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(video_output, fourcc, fps, frame_size)\n",
    "\n",
    "    # Loop through the images in the folder and add them to the video\n",
    "    for filename in image_files:\n",
    "        img_path = os.path.join(frame_folder, filename)\n",
    "        frame = cv2.imread(img_path)\n",
    "\n",
    "        # Write the frame to the video\n",
    "        out.write(frame)\n",
    "\n",
    "    # Release the VideoWriter object\n",
    "    out.release()\n",
    "\n",
    "    print(f\"Video created successfully: {video_output}\")\n",
    "\n",
    "\n",
    "create_video_from_images(\n",
    "    \"results/singer3\",\n",
    "    'singer3_custom.mp4', 15)\n",
    "\n",
    "# create_video_from_images(\n",
    "#     \"debug/hand2\",\n",
    "#     'hand2_debug.mp4', 15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
